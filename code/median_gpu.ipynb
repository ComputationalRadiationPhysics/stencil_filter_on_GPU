{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load needed modules\n",
    "import numba \n",
    "from numba import cuda\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= median ================================== #\n",
    "\n",
    "'''\n",
    " @param:\n",
    "     - input_data:  np.array, 3D data to filter\n",
    "     - output_data: np.array, 3D data after filtering,same size of input (because cuda kernel function can't return value)\n",
    "     - fkt:         function, numba_median, numba_avgerage, ... (what else?) ==> Gaussian?\n",
    "     - stencil:     tuple of size 3, the dimension of 3D stencil \n",
    "     \n",
    " @function: \n",
    "     stencil filter on GPU, reference: https://en.wikipedia.org/wiki/Stencil_code\n",
    "'''\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_smooth_kernel_naive(input_data, output_data, stencil_z, stencil_y, stencil_x, Nz, Ny, Nx):\n",
    "    \n",
    "    # ==== stencil size ==== #\n",
    "    #dx = stencil_[0] dosen't work, compile issue\n",
    "    #dy = stencil_[1]\n",
    "    #dz = stencil_[2]\n",
    "    dx = stencil_x\n",
    "    dy = stencil_y\n",
    "    dz = stencil_z\n",
    "    # ====================== #\n",
    "    \n",
    "    row_init = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    col_init = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    depth_init = cuda.threadIdx.z + cuda.blockIdx.z * cuda.blockDim.z\n",
    "    \n",
    "    # grid stride:\n",
    "    for depth_global in range(depth_init, Nz, cuda.blockDim.z * cuda.gridDim.z):\n",
    "        z_min = max(depth_global - stencil_z, 0)\n",
    "        z_max = min(depth_global + stencil_z + 1, Nz)\n",
    "        for col_global in range(col_init, Ny, cuda.blockDim.y * cuda.gridDim.y):\n",
    "            y_min = max(col_global - stencil_y, 0)\n",
    "            y_max = min(col_global + stencil_y + 1, Ny)\n",
    "            for row_global in range(row_init, Nx, cuda.blockDim.x * cuda.gridDim.x):\n",
    "                \n",
    "                x_min = max(row_global - stencil_x, 0)\n",
    "                x_max = min(row_global + stencil_x + 1, Nx)\n",
    "                \n",
    "                # define local array in cuda: \n",
    "                # https://stackoverflow.com/questions/48642481/what-is-the-correct-usage-of-cuda-local-array-in-numba\n",
    "                sort_array = cuda.local.array(10 * 8 * 10, numba.float32) \n",
    "                \n",
    "                # load in local memory to sort\n",
    "                n = 0\n",
    "                for i in range(z_min,z_max):\n",
    "                    for j in range(y_min,y_max):\n",
    "                        for k in range(x_min,x_max):\n",
    "                            sort_array[n] = input_data[i,j,k]\n",
    "                            n += 1\n",
    "                \n",
    "                # selection sort:\n",
    "                # https://www.cnblogs.com/BobHuang/p/11263183.html <== chinese website\n",
    "                for i in range(n - 1):\n",
    "                    min_index = i;                  \n",
    "                    for j in range(i+1, n):\n",
    "                        if (sort_array[j] < sort_array[min_index]):\n",
    "                            min_index = j; \n",
    "                \n",
    "                    #swap(sort_array[i], sort_array[minIndex]);\n",
    "                    tmp = sort_array[i]\n",
    "                    sort_array[i] = sort_array[min_index]\n",
    "                    sort_array[min_index] = tmp\n",
    "                    \n",
    "                half = int(n / 2)\n",
    "                if (n % 2) == 1:\n",
    "                    median = sort_array[half]\n",
    "                else:\n",
    "                    median = (sort_array[half-1] + sort_array[half]) / 2.0\n",
    "             \n",
    "                output_data[depth_global, col_global, row_global] = median\n",
    "                \n",
    "\n",
    "        \n",
    "def lauch_kernel(input_data, output_data, stencil_):\n",
    "    # TODO: set blocksize, gridsize and lauch kernel\n",
    "    # define threads and blocks\n",
    "    threads = (16, 8, 4)\n",
    "    blocks = (8, 8, 8)\n",
    "    print(\"==================\")\n",
    "    print(\"threads: \" + str(threads))\n",
    "    print(\"blocks: \" + str(blocks))\n",
    "    print(\"==================\")\n",
    "    \n",
    "    Nz, Ny, Nx = np.shape(input_data) # why here z, y, x ?\n",
    "    stencil_z, stencil_y, stencil_x = stencil_[0], stencil_[1], stencil_[2]\n",
    "  \n",
    "    # call CUDA kernel\n",

    "    gpu_smooth_kernel_naive[threads, blocks](input_data, output_data,stencil_z, stencil_y, stencil_x, Nz, Ny, Nx)\n",
    "    \n"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k80_numba",
   "language": "python",
   "name": "k80_numba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
